{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Import the cleaned dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "file_path = 'final_filtered_training_set.npz'\n",
        "\n",
        "with np.load(file_path) as data:\n",
        "    training_images = data['images']\n",
        "    training_labels = data['labels']\n",
        "\n",
        "#Check\n",
        "print(f\"Shape of images: {training_images.shape}\")\n",
        "print(f\"type of images: {type(training_images)}\")\n",
        "print(f\"shape of labels: {training_labels.shape}\")\n",
        "print(f\"type of labels: {type(training_labels)}\")\n",
        "# Dataframe for labels\n",
        "labels = training_labels.flatten()\n",
        "df_labels = pd.DataFrame(labels, columns=['label'])\n",
        "print(df_labels.head())"
      ],
      "metadata": {
        "id": "bdD8K5AhtCYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = df_labels['label'].value_counts()\n",
        "class_percent = 100 * class_counts / len(df_labels)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=class_percent.index.astype(str), y=class_percent.values, palette='magma')\n",
        "print(class_percent.values)\n",
        "plt.title('Class distributions')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F0PvRF5vDtFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train, val, test split\n"
      ],
      "metadata": {
        "id": "Uy3TNEC7ih4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train test val split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define split sizes\n",
        "test_size = 0.2        # 20% for test\n",
        "validation_size = 0.2  # 20% of the remaining 80% = 16% for validation\n",
        "\n",
        "# Split into (training+validation) and test sets\n",
        "training_labels = training_labels.flatten()\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    training_images,\n",
        "    training_labels,\n",
        "    test_size=test_size,\n",
        "    random_state=42,\n",
        "    stratify=training_labels\n",
        ")\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val,\n",
        "    y_train_val,\n",
        "    test_size=validation_size,\n",
        "    random_state=42,\n",
        "    stratify=y_train_val\n",
        ")\n",
        "print(\"Training Images - Min pixel value:\", np.min(X_train))\n",
        "print(\"Training Images - Max pixel value:\", np.max(X_train))\n",
        "print(\"Test Images - Min pixel value:\", np.min(X_test))\n",
        "print(\"Test Images - Max pixel value:\", np.max(X_test))\n",
        "\n",
        "\n",
        "#normalize here, how to do it for inceptionresnetv2? i read it needs normalization betwen [-1,1]\n",
        "\n",
        "\n",
        "#.....\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Validation set shape: {X_val.shape}, {y_val.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}, {y_test.shape}\")\n"
      ],
      "metadata": {
        "id": "6YP-JBROyGwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot class distribution\n",
        "def plot_class_distribution(labels, subset_name):\n",
        "    df = pd.DataFrame({'Label': labels})\n",
        "    plt.figure(figsize=(10,6))\n",
        "    sns.countplot(x='Label', data=df, palette='viridis')\n",
        "    plt.title(f'Class Distribution in {subset_name} Set')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Number of Images')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Plot distributions\n",
        "plot_class_distribution(y_train, 'Training')\n",
        "plot_class_distribution(y_val, 'Validation')\n",
        "plot_class_distribution(y_test, 'Test')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IRQd4wfUyK6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Balancing of the training set, done by oversampling"
      ],
      "metadata": {
        "id": "kmceVfkAimcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#OVERSAMPLING\n",
        "\n",
        "# Determine the maximum class count\n",
        "import numpy as np\n",
        "\n",
        "#class_counts = np.bincount(training_labels)\n",
        "class_counts = pd.Series(y_train).value_counts()  # Use only y_train, not the full label set\n",
        "\n",
        "# `class_counts` now contains the size of each class:\n",
        "# - `class_counts[j]` gives the count of items with class label `j`.\n",
        "\n",
        "max_count = class_counts.max()\n",
        "print(f\"Maximum class count: {max_count}\")\n",
        "\n",
        "# Initialize lists to hold oversampled data\n",
        "X_train_oversampled = []\n",
        "y_train_oversampled = []\n",
        "\n",
        "# Iterate through each class to perform oversampling\n",
        "for class_label in class_counts.index:\n",
        "    # Current class samples\n",
        "    X_class = X_train[y_train == class_label]\n",
        "    y_class = y_train[y_train == class_label]\n",
        "\n",
        "    # Number of samples to add\n",
        "    samples_needed = max_count - len(X_class)\n",
        "\n",
        "    if samples_needed > 0:\n",
        "        # Calculate how many times to duplicate the class samples\n",
        "        duplicates = samples_needed // len(X_class)\n",
        "        remainder = samples_needed % len(X_class)\n",
        "\n",
        "        # Duplicate the entire class as many times as needed\n",
        "        for _ in range(duplicates):\n",
        "            X_train_oversampled.append(X_class)\n",
        "            y_train_oversampled.append(y_class)\n",
        "\n",
        "        # Add the remaining samples by randomly selecting from the class\n",
        "        if remainder > 0:\n",
        "            indices = np.random.choice(len(X_class), size=remainder, replace=True)\n",
        "            X_train_oversampled.append(X_class[indices])\n",
        "            y_train_oversampled.append(y_class[indices])\n",
        "\n",
        "# Concatenate the oversampled data\n",
        "if X_train_oversampled:\n",
        "    X_train_oversampled = np.vstack(X_train_oversampled)\n",
        "    y_train_oversampled = np.hstack(y_train_oversampled)\n",
        "\n",
        "    # Append the oversampled data to the original training set\n",
        "    X_train_balanced = np.vstack((X_train, X_train_oversampled))\n",
        "    y_train_balanced = np.hstack((y_train, y_train_oversampled))\n",
        "else:\n",
        "    # If no oversampling is needed\n",
        "    X_train_balanced = X_train\n",
        "    y_train_balanced = y_train\n",
        "\n",
        "print(f\"Training set shape after oversampling: {X_train_balanced.shape}, {y_train_balanced.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "HcyketE70_XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count classes in training set\n",
        "class_counts = pd.Series(y_train_balanced).value_counts().sort_index()\n",
        "print(\"Class distribution in Training set after oversampling:\")\n",
        "print(class_counts)\n",
        "\n",
        "# Plot for better visualization\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=class_counts.index, y=class_counts.values, palette='viridis')\n",
        "plt.title('Class Distribution in Training Set after Oversampling')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of Images')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "04Zq3yFO0f-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AUGMENTATION template"
      ],
      "metadata": {
        "id": "68gIWbWvi0vN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.core.composition import Compose\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n"
      ],
      "metadata": {
        "id": "kIHLjlzh1fbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the augmentation pipeline\n",
        "augmentation_pipeline = Compose([\n",
        "    #add what type of augmentations you want, example:\n",
        "    #A.CoarseDropout(max_holes=8, max_height=16, max_width=16, min_holes=1, min_height=16, min_width=16, fill_value=0, p=0.5)\n",
        "    #this puts black squares randomly in the images\n",
        "])\n",
        "\n",
        "print(y_train_balanced.dtype)\n",
        "y_train_balanced = y_train_balanced.reshape(-1).astype(np.int32)\n",
        "print(y_train_balanced.dtype)"
      ],
      "metadata": {
        "id": "QDSOy7yu1vEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "def augment_image(image, label):\n",
        "    # Convert TensorFlow tensors to NumPy arrays\n",
        "    image = image.numpy()\n",
        "    label = label.numpy()\n",
        "\n",
        "    # Ensure the image is in uint8 format\n",
        "    if image.dtype != np.uint8:\n",
        "        image = (image * 255).astype(np.uint8)\n",
        "\n",
        "    # Convert RGB to BGR for OpenCV\n",
        "    #image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Apply augmentation\n",
        "    augmented = augmentation_pipeline(image=image)\n",
        "    image = augmented['image']\n",
        "\n",
        "    # Convert back to float32 and normalize\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "    image = (image - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    # Ensure label is int32\n",
        "    label = label.astype(np.int32)\n",
        "\n",
        "    return image, label\n"
      ],
      "metadata": {
        "id": "9RDqsCrJ6fXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wrapper\n",
        "def tf_augment_image(image, label):\n",
        "    # Apply the augment_image function using tf.py_function\n",
        "    augmented_image, augmented_label = tf.py_function(\n",
        "        func=augment_image,\n",
        "        inp=[image, label],\n",
        "        Tout=[tf.float32, tf.int32]  # Corrected label type to int32\n",
        "    )\n",
        "\n",
        "    # Set the shape information\n",
        "    augmented_image.set_shape((96, 96, 3))\n",
        "    augmented_label.set_shape(())\n",
        "\n",
        "    return augmented_image, augmented_label\n",
        "\n"
      ],
      "metadata": {
        "id": "vhFuXLz510Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Create TensorFlow Dataset from balanced training data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_balanced, y_train_balanced))\n",
        "\n",
        "# Apply the augmentation to the training dataset\n",
        "train_dataset = train_dataset.map(\n",
        "    tf_augment_image,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "\n",
        "# Shuffle, batch, and prefetch for performance\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(32).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "RCiqH_Si12NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to display images in a grid\n",
        "def display_augmented_images(dataset, num_images=15, rows=3, cols=5):\n",
        "    for images, labels in dataset.take(1):\n",
        "        fig, axes = plt.subplots(rows, cols, figsize=(10, 6))\n",
        "        fig.tight_layout(pad=2.0)  # Adjust spacing between images\n",
        "\n",
        "        for i in range(num_images):\n",
        "            row, col = divmod(i, cols)\n",
        "            img = images[i].numpy()\n",
        "\n",
        "            # Denormalize the image\n",
        "            img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "            img = np.clip(img, 0, 1)  # Clip to [0, 1] range for display\n",
        "\n",
        "            axes[row, col].imshow(img)\n",
        "            axes[row, col].set_title(f'Label: {labels[i].numpy()}')\n",
        "            axes[row, col].axis('off')\n",
        "\n",
        "        # Hide any remaining empty subplots\n",
        "        for j in range(num_images, rows * cols):\n",
        "            fig.delaxes(axes.flatten()[j])\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "# Display augmented images from the training set in a 3x5 grid\n",
        "display_augmented_images(train_dataset, num_images=15, rows=3, cols=5)\n",
        "\n"
      ],
      "metadata": {
        "id": "yoFpvR4K15Mq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}